% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage[ruled,vlined]{algorithm2e}
\usepackage[left=2cm,right=2cm,top = 2cm,bottom = 2cm]{geometry}
\usepackage[style=alphabetic]{biblatex}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tikz}{}{\usepackage{tikz}}
\makeatother
        \newcommand*\circled[1]{\tikz[baseline=(char.base)]{
          \node[shape=circle,draw,inner sep=1pt] (char) {{\scriptsize#1}};}}  
                  
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{biblatex}
\addbibresource{refs.bib}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Report on SINDy progress},
  pdfauthor={Gage Bonner and Michael Castellucci},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Report on SINDy progress}
\author{Gage Bonner and Michael Castellucci}
\date{}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Summary of work}\label{summary-of-work}

In this project, we investigated the application of \emph{data
discovery} algorithms to learn the governing equations for a dynamical
system from numerical realizations of their trajectories. We apply the
sparse identification of nonlinear dynamics (``SINDy'') as originally
proposed in \cite{brunton2016discovering} as well as a number of its
extensions. The core idea behind these algorithms is that many dynamical
systems in science are represented by differential equation systems
\(\dot{x} = f(x)\) where \(f(x)\) is often a linear combination of a
small number of elementary functions. By contrast, the trajectories
\(x(t)\) may be extremely complicated and ill-suited to direct fitting.
Consider the classic Lorenz system
\begin{subequations} \label{eq:lorenz-def}
\begin{align} 
    \dot{x} &= 10 (y - x), \\
    \dot{y} &= x (28 - z) - y, \\ 
    \dot{z} &= x y - (8 / 3) z .
\end{align}
\end{subequations} Trajectories \(X(t) = (x(t), y(t), z(t))\) of this
system are chaotic, making it difficult or impossible to propose a
function that fits \(X(t)\) directly. On the other hand, \(\dot{X}\) is
a low order polynomial function of \(X(t)\) and hence should be much
more tractable in principle. At the highest level, we therefore have the
following problem: given numerical trajectories
\(\{x(t_1), x(t_2), \dots \}\), compute numerically
\(\{\dot{x}(t_1), \dot{x}(t_2), \dots \}\) and attempt to find a
parsimonious (equivalently: \emph{sparse}) combination of simple
functions of the \(x(t)\) that faithfully represents it.

This report contains embedded code from the
\href{https://julialang.org/}{Julia} programming language.

\section{Core algorithms}\label{core-algorithms}

\subsection{Sparse representations}\label{sparse-representations}

Suppose that several values of a function of
\(f : \mathbb{R} \to \mathbb{R}\) are observed,
\(\{f(x_1), f(x_2), \dots \}\). A \emph{sparse representation} seeks to
represent \(f\) as a linear combination of elementary functions of \(x\)
that contains as few terms as possible while still faithfully
representing the behavior of \(f\). As a first example, take
\(f(x) = x + \sin(x)\). We will add a small noise term to simulate real
data. This is shown in Figure \ref{fig:f-llsq}.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{Random}\NormalTok{.}\FunctionTok{seed!}\NormalTok{(}\FloatTok{1234}\NormalTok{)}
\FunctionTok{f\_simple}\NormalTok{(x) }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+} \FunctionTok{sin}\NormalTok{(x)}
\NormalTok{x\_points\_f\_simple }\OperatorTok{=} \FunctionTok{range}\NormalTok{(}\OperatorTok{{-}}\FloatTok{5}\NormalTok{, }\FloatTok{5}\NormalTok{, length }\OperatorTok{=} \FloatTok{20}\NormalTok{) }\OperatorTok{|\textgreater{}}\NormalTok{ collect}
\NormalTok{f\_simple\_points }\OperatorTok{=}\NormalTok{ [}\FunctionTok{f\_simple}\NormalTok{(x) }\OperatorTok{+} \FloatTok{0.2}\FunctionTok{*rand}\NormalTok{() for x }\KeywordTok{in}\NormalTok{ x\_points\_f\_simple]}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\begin{subfigure}{0.49\textwidth}
    \includegraphics{figures/f-llsq.png}
    \caption{The function $f(x) = x + \sin(x) + \text{noise}$ and the linear least-squares fit $f_\text{llsq}(x)$ of Eq. \eqref{eq:f-1-llsq}.}
    \label{fig:f-llsq}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \includegraphics{figures/g-ridge.png}
    \caption{The function $g(x) = x + \sin(x) + \cos^2(x) + \text{noise}$ and the ridge regression fit $g_\text{ridge}(x)$ of Eq. \eqref{eq:g-ridge}.}
    \label{fig:g-ridge}
\end{subfigure}
        
\caption{Fitting of two simple functions using linear least-squares and ridge regression.}
\label{fig:sparse-motivation}
\end{figure}

Assuming now that we are given this data with no knowledge of the
underlying mechanics, how could this function be represented? Due to the
oscillatory nature of the function, we might assume that it may be some
linear combination of simple polynomials and sinusoids. Provided this
sort of ``expert knowledge'', we could propose a \emph{library} of
functions \(L(x)\) that constitute the set of all possible functions
that we want to include in our model. In our case, we will take our
library to be the vector \begin{equation}
L(x) = (1, x, x^2, x^3, \sin x, \cos x).
\end{equation} The problem is therefore to find a vector
\(\xi \in \mathbb{R}^7\) such that \(f(x) \approx \xi \cdot L(x)\). To
do this, we will construct an optimization problem whose solution is
\(\xi\) that takes all of our observational data into account. The
\emph{library data} \(\Theta\) is given by \begin{equation}
\Theta(x) 
= 
\begin{pmatrix}
L_1(x_1) & L_2(x_1) & \cdots & L_7(x_1) \\ 
L_1(x_2) & L_2(x_2) & \cdots & L_7(x_2) \\
\vdots   & \vdots   & \ddots & \vdots   \\
L_1(x_N) & L_2(x_N) & \cdots & L_7(x_N) 
\end{pmatrix}
=
\begin{pmatrix}
1 & x_1 & \cdots & \cos(x_1) \\ 
1 & x_2 & \cdots & \cos(x_2) \\
\vdots   & \vdots   & \ddots & \vdots   \\
1 & x_N & \cdots & \cos(x_N) 
\end{pmatrix}.
\end{equation} Given our \emph{target data}
\(F = (f(x_1), f(x_2), \dots)\), we naturally have the following
optimization problem \begin{equation}
\xi^\star = \underset{\xi \in \mathbb{R}^7}{\text{argmin}} \lVert F - \Theta \xi \rVert_{2} ,
\end{equation} where \(\Vert\cdot\rVert_2\) indicates the \(L_2\) norm
and where our final representation is
\(f(x) \approx L(x) \cdot \xi^\star\). This kind of simple problem is
directly amenable to a linear least-squares solution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{library }\OperatorTok{=}\NormalTok{ [x }\OperatorTok{{-}\textgreater{}} \FloatTok{1.0}\NormalTok{, x }\OperatorTok{{-}\textgreater{}}\NormalTok{ x, x }\OperatorTok{{-}\textgreater{}}\NormalTok{ x}\OperatorTok{\^{}}\FloatTok{2}\NormalTok{, x }\OperatorTok{{-}\textgreater{}}\NormalTok{ x}\OperatorTok{\^{}}\FloatTok{3}\NormalTok{, x }\OperatorTok{{-}\textgreater{}} \FunctionTok{sin}\NormalTok{(x), x }\OperatorTok{{-}\textgreater{}} \FunctionTok{cos}\NormalTok{(x)]}
\NormalTok{library\_names }\OperatorTok{=}\NormalTok{ [}\StringTok{"1"}\NormalTok{, }\StringTok{"x"}\NormalTok{, }\StringTok{"x\^{}2"}\NormalTok{, }\StringTok{"x\^{}3"}\NormalTok{, }\StringTok{"sin(x)"}\NormalTok{, }\StringTok{"cos(x)"}\NormalTok{]}
\NormalTok{library\_data }\OperatorTok{=}\NormalTok{ [}\FunctionTok{f}\NormalTok{(t) for t }\KeywordTok{in}\NormalTok{ x\_points\_f\_simple, f }\KeywordTok{in}\NormalTok{ library]}

\NormalTok{ls }\OperatorTok{=} \FunctionTok{llsq}\NormalTok{(library\_data, f\_simple\_points, bias }\OperatorTok{=} \ConstantTok{false}\NormalTok{) }\CommentTok{\# from MultivariateStats.jl}
\FunctionTok{f\_llsq\_1}\NormalTok{(x) }\OperatorTok{=} \FunctionTok{sum}\NormalTok{(ls[i]}\OperatorTok{*}\NormalTok{library[i](x) }\ControlFlowTok{for}\NormalTok{ i }\OperatorTok{=} \FloatTok{1}\OperatorTok{:}\FunctionTok{length}\NormalTok{(ls))}

\NormalTok{f\_simple\_pred }\OperatorTok{=}\NormalTok{ library\_data }\OperatorTok{*}\NormalTok{ ls}
\NormalTok{rmse }\OperatorTok{=} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{mean}\NormalTok{(}\FunctionTok{abs2}\NormalTok{.(f\_simple\_points }\OperatorTok{{-}}\NormalTok{ f\_simple\_pred)))}
\end{Highlighting}
\end{Shaded}

\begin{subequations} \label{eq:f-1-llsq} \begin{align}    f_{\text{llsq}}(x) &= 0.111 \cdot 1 + 1.01 \cdot x + 0.000703 \cdot x^{2} -0.000482 \cdot x^{3} + 0.946 \cdot \sin\left( x \right) + 0.000711 \cdot \cos\left( x \right) \\
  \text{RMSE}[f_{\text{llsq}}] &= 0.0421 \end{align} \end{subequations}

We can see that the fit produced is indeed quite good as in Figure
\ref{fig:f-llsq}. However, \(f_{\text{llsq}}\) is not sparse, that is,
it contains more terms than it necessarily needs to and, in particular,
many terms that have small coefficients. Examining Eq.
\eqref{eq:f-1-llsq}, we see that the coefficient of \(x^3\) is very
small. We might therefore try removing it from the library and applying
another linear least-squares fit.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{library }\OperatorTok{=}\NormalTok{ [x }\OperatorTok{{-}\textgreater{}} \FloatTok{1.0}\NormalTok{, x }\OperatorTok{{-}\textgreater{}}\NormalTok{ x, x }\OperatorTok{{-}\textgreater{}}\NormalTok{ x}\OperatorTok{\^{}}\FloatTok{2}\NormalTok{, x }\OperatorTok{{-}\textgreater{}} \FunctionTok{sin}\NormalTok{(x), x }\OperatorTok{{-}\textgreater{}} \FunctionTok{cos}\NormalTok{(x)]}
\NormalTok{library\_names }\OperatorTok{=}\NormalTok{ [}\StringTok{"1"}\NormalTok{, }\StringTok{"x"}\NormalTok{, }\StringTok{"x\^{}2"}\NormalTok{, }\StringTok{"sin(x)"}\NormalTok{, }\StringTok{"cos(x)"}\NormalTok{]}
\NormalTok{library\_data }\OperatorTok{=}\NormalTok{ [}\FunctionTok{f}\NormalTok{(t) for t }\KeywordTok{in}\NormalTok{ x\_points\_f\_simple, f }\KeywordTok{in}\NormalTok{ library]}

\NormalTok{ls }\OperatorTok{=} \FunctionTok{llsq}\NormalTok{(library\_data, f\_simple\_points, bias }\OperatorTok{=} \ConstantTok{false}\NormalTok{)}
\FunctionTok{f\_llsq\_2}\NormalTok{(x) }\OperatorTok{=} \FunctionTok{sum}\NormalTok{(ls[i]}\OperatorTok{*}\NormalTok{library[i](x) }\ControlFlowTok{for}\NormalTok{ i }\OperatorTok{=} \FloatTok{1}\OperatorTok{:}\FunctionTok{length}\NormalTok{(ls))}

\NormalTok{f\_simple\_pred }\OperatorTok{=}\NormalTok{ library\_data }\OperatorTok{*}\NormalTok{ ls}
\NormalTok{rmse }\OperatorTok{=} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{mean}\NormalTok{(}\FunctionTok{abs2}\NormalTok{.(f\_simple\_points }\OperatorTok{{-}}\NormalTok{ f\_simple\_pred)))}
\end{Highlighting}
\end{Shaded}

\begin{subequations} \label{eq:f-2-llsq} \begin{align}    f_{\text{llsq, 2}}(x) &= 0.111 \cdot 1 + 1 \cdot x + 0.000703 \cdot x^{2} + 0.958 \cdot \sin\left( x \right) + 0.000711 \cdot \cos\left( x \right) \\
    \text{RMSE}[f_{\text{llsq}}] &= 0.0425 \end{align} \end{subequations}

This has produced a more parsimonious representation and comparing Eqs.
\eqref{eq:f-1-llsq} and \eqref{eq:f-2-llsq} shows that the RSME has
increased by a negligible amount.

This calculation contains the essence of the basic sparse representation
algorithm: iteratively regress library functions onto the target data
and remove small coefficients. Before moving on, we will consider one
more example which demonstrates the need for a more advanced regression
tool than the basic linear least-squares. In this case we will take
\(g(x) = x + \sin(x) + \cos(x)^2\), see Figure \ref{fig:g-ridge}.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{Random}\NormalTok{.}\FunctionTok{seed!}\NormalTok{(}\FloatTok{1234}\NormalTok{)}
\FunctionTok{g\_simple}\NormalTok{(x) }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+} \FunctionTok{sin}\NormalTok{(x) }\OperatorTok{+} \FunctionTok{cos}\NormalTok{(x)}\OperatorTok{\^{}}\FloatTok{2}
\NormalTok{x\_points\_g }\OperatorTok{=} \FunctionTok{range}\NormalTok{(}\OperatorTok{{-}}\FloatTok{5}\NormalTok{, }\FloatTok{5}\NormalTok{, length }\OperatorTok{=} \FloatTok{20}\NormalTok{) }\OperatorTok{|\textgreater{}}\NormalTok{ collect}
\NormalTok{g\_points }\OperatorTok{=}\NormalTok{ [}\FunctionTok{g\_simple}\NormalTok{(x) }\OperatorTok{+} \FloatTok{0.2}\FunctionTok{*rand}\NormalTok{() for x }\KeywordTok{in}\NormalTok{ x\_points\_g]}
\end{Highlighting}
\end{Shaded}

Using expert knowledge of \(g(x)\), we will augment our library with
squared sinusoids. We might attempt the following solution as per the
earlier calculations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{library }\OperatorTok{=}\NormalTok{ [}
\NormalTok{  x }\OperatorTok{{-}\textgreater{}} \FloatTok{1.0}\NormalTok{, x }\OperatorTok{{-}\textgreater{}}\NormalTok{ x, x }\OperatorTok{{-}\textgreater{}}\NormalTok{ x}\OperatorTok{\^{}}\FloatTok{2}\NormalTok{, x }\OperatorTok{{-}\textgreater{}}\NormalTok{ x}\OperatorTok{\^{}}\FloatTok{3}\NormalTok{, }
\NormalTok{  x }\OperatorTok{{-}\textgreater{}} \FunctionTok{sin}\NormalTok{(x), x }\OperatorTok{{-}\textgreater{}} \FunctionTok{cos}\NormalTok{(x), }
\NormalTok{  x }\OperatorTok{{-}\textgreater{}} \FunctionTok{sin}\NormalTok{(x)}\OperatorTok{\^{}}\FloatTok{2}\NormalTok{, x }\OperatorTok{{-}\textgreater{}} \FunctionTok{cos}\NormalTok{(x)}\OperatorTok{\^{}}\FloatTok{2}\NormalTok{]}
\NormalTok{library\_names }\OperatorTok{=}\NormalTok{ [}\StringTok{"1"}\NormalTok{, }\StringTok{"x"}\NormalTok{, }\StringTok{"x\^{}2"}\NormalTok{, }\StringTok{"x\^{}3"}\NormalTok{, }\StringTok{"sin(x)"}\NormalTok{, }\StringTok{"cos(x)"}\NormalTok{, }\StringTok{"sin(x)\^{}2"}\NormalTok{, }\StringTok{"cos(x)\^{}2"}\NormalTok{]}
\NormalTok{library\_data }\OperatorTok{=}\NormalTok{ [}\FunctionTok{f}\NormalTok{(t) for t }\KeywordTok{in}\NormalTok{ x\_points\_g, f }\KeywordTok{in}\NormalTok{ library]}

\CommentTok{\# llsq(library\_data, g\_sample\_points, bias = false) \# will error!}
\end{Highlighting}
\end{Shaded}

The above computation will result in an error on the \texttt{llsq}
calculation. The culprits are the functions \(\sin^2(x)\) and
\(\cos^2(x)\); they are linearly dependent due to
\(\sin^2(x) + \cos^2(x) = 1\). The net result is that the library data
matrix is ill-conditioned for basic least-squares. One might wonder why
we would include both \(\sin^2(x)\) and \(\cos^2(x)\) in the first
place. Here we should recall that the general problem is to find the
sparsest representation, i.e.~if we can use \(\cos^2(x)\) rather than
\(\sin^2(x) - 1\), this is preferable. One popular solution to the issue
of linearly dependent library data is \emph{ridge regression}
\cite{hoerl1970ridge} which introduces an additional parameter
\(\lambda\) and considers the optimization problem \begin{equation}
\xi^\star = \underset{\xi}{\text{argmin}}\bigg[ \lVert F - \Theta \xi \rVert_{2} + \lambda \lVert I \xi \rVert_{2} \bigg],
\end{equation} where \(I\) is the identity matrix. We can apply this
ridge regression with \(\lambda = 0.1\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lambda\_ridge }\OperatorTok{=} \FloatTok{0.1}
\NormalTok{rr\_g }\OperatorTok{=} \FunctionTok{ridge}\NormalTok{(library\_data, g\_points, lambda\_ridge, bias }\OperatorTok{=} \ConstantTok{false}\NormalTok{) }\CommentTok{\# from MultivariateStats.jl}
\FunctionTok{g\_ridge}\NormalTok{(x) }\OperatorTok{=} \FunctionTok{sum}\NormalTok{(rr\_g[i]}\OperatorTok{*}\NormalTok{library[i](x) }\ControlFlowTok{for}\NormalTok{ i }\OperatorTok{=} \FloatTok{1}\OperatorTok{:}\FunctionTok{length}\NormalTok{(rr\_g))}

\NormalTok{g\_pred }\OperatorTok{=}\NormalTok{ library\_data }\OperatorTok{*}\NormalTok{ rr\_g}
\NormalTok{rmse }\OperatorTok{=} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{mean}\NormalTok{(}\FunctionTok{abs2}\NormalTok{.(g\_points }\OperatorTok{{-}}\NormalTok{ g\_pred)))}
\end{Highlighting}
\end{Shaded}

\begin{subequations} \label{eq:g-ridge} \scriptsize \begin{align}    g_{\text{ridge}}(x) &= 0.403 \cdot 1 + 1.01 \cdot x + 0.00129 \cdot x^{2} -0.000763 \cdot x^{3} + 0.929 \cdot \sin\left( x \right) + 0.00409 \cdot \cos\left( x \right) -0.307 \cdot \sin^{2}\left( x \right) + 0.71 \cdot \cos^{2}\left( x \right) \\
    \text{RMSE}[g_{\text{ridge}}] &= 0.0417 \end{align} \end{subequations}

We obtain a good fit as shown in Figure \ref{fig:g-ridge}, but it is not
sparse. With these motivating examples in hand, we can outline the
\emph{sequentially-thresholded ridge regression} (STRidge) algorithm.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, rightrule=.15mm, colback=white, coltitle=black, bottomtitle=1mm, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, bottomrule=.15mm, colframe=quarto-callout-tip-color-frame, titlerule=0mm, left=2mm, title={STRidge algorithm for sparse representation (Julia implementation)}, arc=.35mm, opacityback=0]

\phantomsection\label{stridge}
\phantomsection\label{annotated-cell-9}%
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{function} \FunctionTok{STRidge}\NormalTok{(}
\NormalTok{  target\_data}\OperatorTok{::}\DataTypeTok{Vector\{\textless{}:Real\}}\NormalTok{, }\hspace*{\fill}\NormalTok{\circled{1}}
\NormalTok{  library\_data}\OperatorTok{::}\DataTypeTok{Matrix\{\textless{}:Real\}}\NormalTok{; }\hspace*{\fill}\NormalTok{\circled{2}}
\NormalTok{  lambda\_sparse}\OperatorTok{::}\DataTypeTok{Real }\OperatorTok{=} \FloatTok{0.1}\NormalTok{, }\hspace*{\fill}\NormalTok{\circled{3}}
\NormalTok{  lambda\_ridge}\OperatorTok{::}\DataTypeTok{Real }\OperatorTok{=} \FloatTok{0.1}\NormalTok{, }\hspace*{\fill}\NormalTok{\circled{4}}
\NormalTok{  max\_iters}\OperatorTok{::}\DataTypeTok{Integer }\OperatorTok{=} \FloatTok{10}\NormalTok{)}

  \FunctionTok{rr}\NormalTok{(data) }\OperatorTok{=} \FunctionTok{ridge}\NormalTok{(data, target\_data, lambda\_ridge, bias }\OperatorTok{=} \ConstantTok{false}\NormalTok{) }\hspace*{\fill}\NormalTok{\circled{5}}
\NormalTok{  Xi }\OperatorTok{=} \FunctionTok{rr}\NormalTok{(library\_data)}

  \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \FloatTok{1}\OperatorTok{:}\NormalTok{max\_iters}
\NormalTok{      smallinds }\OperatorTok{=} \FunctionTok{findall}\NormalTok{(p }\OperatorTok{{-}\textgreater{}} \FunctionTok{abs}\NormalTok{(p) }\OperatorTok{\textless{}}\NormalTok{ lambda\_sparse, Xi)}
\NormalTok{      Xi[smallinds] }\OperatorTok{.=} \FloatTok{0}
\NormalTok{      biginds }\OperatorTok{=} \FunctionTok{setdiff}\NormalTok{(}\FloatTok{1}\OperatorTok{:}\FunctionTok{length}\NormalTok{(Xi), smallinds)}
\NormalTok{      Xi[biginds] }\OperatorTok{=} \FunctionTok{rr}\NormalTok{(library\_data[}\OperatorTok{:}\NormalTok{, biginds])}
  \ControlFlowTok{end}

  \ControlFlowTok{return}\NormalTok{ Xi}
\KeywordTok{end}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
\texttt{target\_data} is a vector of function (observation) values of
length \(N\).
\item[\circled{2}]
\texttt{library\_data} is a matrix with a number of columns equal to the
number of library functions. Then, each column has \(N\) rows, one for
the library function evaluated at each observation.
\item[\circled{3}]
\texttt{lambda\_sparse} is the threshold below which library
coefficients are set equal to zero.
\item[\circled{4}]
\texttt{lambda\_ridge} is the ridge regression regularization parameter.
\item[\circled{5}]
\texttt{ridge} is an implementation the ridge regression algorithm from
\href{https://juliastats.org/MultivariateStats.jl/stable/lreg/\#Ridge-Regression}{\texttt{MultivariateStats.jl}}.
\end{description}

\end{tcolorbox}

We see that the core idea behind the \texttt{STRidge} function is simply
to iteratively threshold small library coefficients and ridge regress
the remaining library functions onto the target data. Let us apply this
to the \(g(x) = x + \sin(x) + \cos^2(x)\) case as above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g\_strr }\OperatorTok{=} \FunctionTok{STRidge}\NormalTok{(g\_points, library\_data, lambda\_sparse }\OperatorTok{=} \FloatTok{0.3}\NormalTok{, lambda\_ridge }\OperatorTok{=} \FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{subequations} \label{eq:g-strr} \begin{align}    g_{\text{strr}}(x) &= 1 \cdot x + 0.948 \cdot \sin\left( x \right) + 1.15 \cdot \cos^{2}\left( x \right) \\
    \text{RMSE}[g_{\text{strr}}] &= 0.0417 \end{align} \end{subequations}

Comparing Eq. \eqref{eq:g-strr} to \eqref{eq:g-ridge} shows that the
\texttt{STRidge} algorithm has resulted in a significantly more sparse
representation without increasing the rmse appreciably. Note that
\(\lambda_\text{sparse}\) and \(\lambda_\text{ridge}\) are not
prescribed currently. One has to choose their values judiciously to
simultaneously remove small terms and avoid removing large terms.

We will address one final issue before proceding to our discussion of
SINDy. What if the target data or observations are multidimensional? As
we are restricting our focus to dynamical systems, multidimensional
observations require no further calculation as all coordinates will be
functions of time. For example, a function such as
\(f(x, y) = x^2 + y^2\) can be sparely represented by using library
functions such as \(x(t), x^2(t) \dots, y(t), y^2(t), \dots\) which
amounts to increasing the number of columns in \(\Theta\). To handle
multidimensional observations, we simply apply the \texttt{STRidge}
algorithm to each dimension of the target data.

\begin{tcolorbox}[enhanced jigsaw, breakable, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, rightrule=.15mm, colback=white, coltitle=black, bottomtitle=1mm, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, bottomrule=.15mm, colframe=quarto-callout-tip-color-frame, titlerule=0mm, left=2mm, title={STRidge algorithm extension to multidimensional observations (Julia
implementation)}, arc=.35mm, opacityback=0]

\phantomsection\label{stridge-vec}
\phantomsection\label{annotated-cell-10}%
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{function} \FunctionTok{STRidge}\NormalTok{(}
\NormalTok{  target\_data}\OperatorTok{::}\DataTypeTok{Matrix\{\textless{}:Real\}}\NormalTok{, }\hspace*{\fill}\NormalTok{\circled{1}}
\NormalTok{  library\_data}\OperatorTok{::}\DataTypeTok{Matrix\{\textless{}:Real\}}\NormalTok{; }
\NormalTok{  lambda\_sparse}\OperatorTok{::}\DataTypeTok{Real }\OperatorTok{=} \FloatTok{0.1}\NormalTok{,}
\NormalTok{  lambda\_ridge}\OperatorTok{::}\DataTypeTok{Real }\OperatorTok{=} \FloatTok{0.1}\NormalTok{,}
\NormalTok{  max\_iters}\OperatorTok{::}\DataTypeTok{Integer }\OperatorTok{=} \FloatTok{10}\NormalTok{)}

\NormalTok{    Xi }\OperatorTok{=} \FunctionTok{zeros}\NormalTok{(}\FunctionTok{size}\NormalTok{(library\_data, }\FloatTok{2}\NormalTok{), }\FunctionTok{size}\NormalTok{(target\_data, }\FloatTok{2}\NormalTok{))}
    \ControlFlowTok{for}\NormalTok{ i }\OperatorTok{=} \FloatTok{1}\OperatorTok{:}\FunctionTok{size}\NormalTok{(target\_data, }\FloatTok{2}\NormalTok{)}
\NormalTok{        sr }\OperatorTok{=} \FunctionTok{STRidge}\NormalTok{(target\_data[}\OperatorTok{:}\NormalTok{, i], library\_data, }
\NormalTok{            lambda\_sparse }\OperatorTok{=}\NormalTok{ lambda\_sparse, }
\NormalTok{            lambda\_ridge }\OperatorTok{=}\NormalTok{ lambda\_ridge,}
\NormalTok{            max\_iters }\OperatorTok{=}\NormalTok{ max\_iters) }\hspace*{\fill}\NormalTok{\circled{2}}
\NormalTok{        Xi[}\OperatorTok{:}\NormalTok{,i] }\OperatorTok{.=}\NormalTok{ sr}
    \ControlFlowTok{end}

    \ControlFlowTok{return}\NormalTok{ Xi}
\KeywordTok{end}
\end{Highlighting}
\end{Shaded}

\begin{description}
\tightlist
\item[\circled{1}]
\texttt{target\_data} is a now a matrix with \(N\) rows and a number of
columns equal to the dimension of the target.
\item[\circled{2}]
We call the scalar version of \texttt{STRidge} on each column of
\texttt{target\_data}.
\end{description}

\end{tcolorbox}

We will apply this to multidimensional target data using our \(f(x)\)
and \(g(x)\) functions of Figure \ref{fig:sparse-motivation}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{target\_data }\OperatorTok{=}\NormalTok{ [f\_simple\_points ;; g\_points]}
\NormalTok{fg\_strr }\OperatorTok{=} \FunctionTok{STRidge}\NormalTok{(target\_data, library\_data, lambda\_sparse }\OperatorTok{=} \FloatTok{0.3}\NormalTok{, lambda\_ridge }\OperatorTok{=} \FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{subequations} \label{eq:g-strr} \begin{align}    f_{\text{strr}}(x) &= 1 \cdot x + 0.948 \cdot \sin\left( x \right) \\
  g_{\text{strr}}(x) &= 1 \cdot x + 0.948 \cdot \sin\left( x \right) + 1.15 \cdot \cos^{2}\left( x \right) \end{align} \end{subequations}

These are the correct sparse representations.

\subsection{SINDy}\label{sindy}

\subsection{E-SINDy}\label{e-sindy}

\section{Julia Implementation}\label{julia-implementation}

\section{Results}\label{results}

\subsection{Direct sparse
representations}\label{direct-sparse-representations}

\subsection{Fluid}\label{fluid}

\subsection{Slow}\label{slow}

\subsection{Full}\label{full}

\section{Conclusions}\label{conclusions}


\printbibliography


\end{document}
