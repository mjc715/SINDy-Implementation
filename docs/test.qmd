---
title: "Report on SINDy progress"
author: "Gage Bonner and Michael Castellucci"
latex-tinytex: true
format:
  pdf:
    documentclass: article
    # classoption: [twocolumn]
    keep-tex: true
    toc: true
    number-sections: true
    colorlinks: true
    include-in-header:
      - text: |
          \usepackage{algorithm}
          \usepackage{algpseudocode}
          \usepackage[left=2cm,right=2cm,top = 2cm,bottom = 2cm]{geometry}
          \usepackage[style=alphabetic]{biblatex}
    cite-method: biblatex
# format:
#   html:
#     toc: true
#     html-math-method: mathml
bibliography: refs.bib
engine: julia
julia:
  exeflags: ["--project=/Users/gagebonner/Desktop/Repositories/SINDy-Implementation/"]
highlight-style: arrow
---

***

```{julia}
#| output: false
#| echo: false
include(joinpath(@__DIR__, "..", "sindy2.jl"))
using CairoMakie; set_theme!(theme_latexfonts())
```

# Summary of work

In this project, we investigated the application of *data discovery* algorithms to learn the governing equations for a dynamical system from numerical realizations of their trajectories. We apply the sparse identification of nonlinear dynamics (``SINDy") as originally proposed in \cite{brunton2016discovering} as well as a number of its extensions. The core idea behind these algorithms is that many dynamical systems in science are represented by differential equation systems $\dot{x} = f(x)$ where $f(x)$ often a linear combination of a small number of elementary functions. By contrast, the trajectories $x(t)$ may be extremely complicated and ill-suited to direct fitting. Consider the classic Lorenz system
\begin{subequations} \label{eq:lorenz-def}
\begin{align} 
    \dot{x} &= 10 (y - x), \\
    \dot{y} &= x (28 - z) - y, \\ 
    \dot{z} &= x y - (8 / 3) z .
\end{align}
\end{subequations}
Trajectories $X(t) = (x(t), y(t), z(t))$ of this system are chaotic, making it difficult or impossible to propose a function that fits $X(t)$ directly. On the other hand, $\dot{X}$ is a low order polynomial function of $X(t)$ and hence should be much more tractable in principle. At the highest level, we therefore have the following problem: given numerical trajectories $\{x(t_1), x(t_2), \dots \}$, compute numerically $\{\dot{x}(t_1), \dot{x}(t_2), \dots \}$ and attempt to find a parsimonious (equivalently: \emph{sparse}) combination of simple functions of the $x(t)$ that faithfully represents it. 

This report contains embedded code from the [Julia](https://julialang.org/) programming language.

# Core algorithms

## Sparse representations

Suppose that several values of a function of $f : \mathbb{R} \to \mathbb{R}$ are observed, $\{f(x_1), f(x_2), \dots \}$. A \emph{sparse representation} seeks to represent $f$ as a linear combination of elementary functions of $x$ that contains as few terms as possible while still faithfully representing the behavior of $f$. As a first example, take $f(x) = x + \sin(x)$. We will add a small noise term to simulate real data. This is shown in Figure \ref{fig:f-1}.

```{julia}
#| output: false
Random.seed!(1234)
f_simple_1(x) = x + sin(x)
x_points_f_simple = range(-5, 5, length = 20) |> collect
f_simple_points = [f_simple_1(x) + 0.2*rand() for x in x_points_f_simple]
```

```{julia}
#| output: false
#| echo: false
let
fig = Figure(fontsize = 25)
ax = Axis(fig[1, 1], xlabel = L"x", ylabel = L"f(x) = x + \sin(x)", aspect = AxisAspect(1))
scatter!(ax, x_points_f_simple, f_simple_points, color = :black,  markersize = 15)
save(joinpath(@__DIR__, "figures", "f-1.png"), fig)
end
```

\begin{figure}
    \centering
    \includegraphics[width = 0.5\textwidth]{figures/f-1.png}
    \caption{A simple data set from a scalar function.}
    \label{fig:f-1}
\end{figure}

Assuming now that we are given this data with no knowledge of the underlying mechanics, how could this function be represented? Due to the oscillatory nature of the function, we might assume that it may be some linear combination of simple polynomials and sinusoids. Provided this sort of "expert knowledge", we could propose a \emph{library} of functions $L(x)$ that constitute the set of all possible functions that we want to include in our model. In our case, we will take our library to be the vector
\begin{equation}
L(x) = (1, x, x^2, x^3, \sin x, \cos x).
\end{equation}
The problem is therefore to find a vector $\xi \in \mathbb{R}^7$ such that $f(x) \approx \xi \cdot L(x)$. To do this, we will construct an optimization problem whose solution is $\xi$ that takes all of our observational data into account. The \emph{library data} $\Theta$ is given by
\begin{equation}
\Theta(x) 
= 
\begin{pmatrix}
L_1(x_1) & L_2(x_1) & \cdots & L_7(x_1) \\ 
L_1(x_2) & L_2(x_2) & \cdots & L_7(x_2) \\
\vdots   & \vdots   & \ddots & \vdots   \\
L_1(x_N) & L_2(x_N) & \cdots & L_7(x_N) 
\end{pmatrix}
=
\begin{pmatrix}
1 & x_1 & \cdots & \cos(x_1) \\ 
1 & x_2 & \cdots & \cos(x_2) \\
\vdots   & \vdots   & \ddots & \vdots   \\
1 & x_N & \cdots & \cos(x_N) 
\end{pmatrix}.
\end{equation}
Given our \emph{target data} $F = (f(x_1), f(x_2), \dots)$, we naturally have the following optimization problem
\begin{equation}
\xi^\star = \underset{\xi \in \mathbb{R}^7}{\text{argmin}} \lVert F - \Theta \xi \rVert_{2} ,
\end{equation}
where $\Vert\cdot\rVert_2$ indicates the $L_2$ norm and where our final representation is $f(x) \approx L(x) \cdot \xi^\star$. This kind of simple problem is directly amenable to a linear least-squares solution.

```{julia}
#| output: false
library_1 = [x -> 1.0, x -> x, x -> x^2, x -> x^3, x -> sin(x), x -> cos(x)]
library_names_1 = ["1", "x", "x^2", "x^3", "sin(x)", "cos(x)"]
library_data_1 = [f(t) for t in x_points_f_simple, f in library_1]

ls_1 = llsq(library_data_1, f_simple_points, bias = false)
f_llsq_1(x) = sum(ls_1[i]*library_1[i](x) for i = 1:length(ls_1))

f_simple_pred = library_data_1 * ls_1
rmse_1 = sqrt(mean(abs2.(f_simple_points - f_simple_pred)))
```

```{julia}
#| echo: false
let
pp_ls = pretty_print(ls_1, library_names_1)

md"""
\begin{subequations} \label{eq:f-1-llsq}
\begin{align} 
  f\_{\text{llsq}}(x) &= $(latexify(pp_ls, env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
  \text{RMSE}[f\_{\text{llsq}}] &= $(latexify(rmse_1, env = :raw, fmt  = FancyNumberFormatter(3)))
\end{align}
\end{subequations}
"""
end
```



We can see that the fit produced is indeed quite good as in Figure \ref{fig:f-1-llsq}.

```{julia}
#| output: false
#| echo: false
let
x = x_points_f_simple
f = [f_llsq_1(x) for x in x_points_f_simple]

fig = Figure(fontsize = 25)
ax = Axis(fig[1, 1], xlabel = L"x", ylabel = L"f", aspect = AxisAspect(1))
scatter!(ax, x_points_f_simple, f_simple_points, color = :black, markersize = 15, label = L"f(x)")
lines!(ax, x, f, color = :red,  linewidth = 4, label = L"f_{\text{llsq}}(x)")
axislegend(ax, position = :lt)
save(joinpath(@__DIR__, "figures", "f-1-llsq.png"), fig)
end
```

\begin{figure}
    \centering
    \includegraphics[width = 0.5\textwidth]{figures/f-1-llsq.png}
    \caption{The linear least squares result.}
    \label{fig:f-1-llsq}
\end{figure}

However, $f_{\text{llsq}}$ is not sparse, that is, it contains more terms than it necessarily needs to and, in particular, many terms that have small coefficients. Examining Eq. \eqref{eq:f-1-llsq}, we see that the coefficient of $x^3$ is very small. We might therefore try removing it from the library and applying another linear least-squares fit.

```{julia}
#| output: false
library_2 = [x -> 1.0, x -> x, x -> x^2, x -> sin(x), x -> cos(x)]
library_names_2 = ["1", "x", "x^2", "sin(x)", "cos(x)"]
library_data_2 = [f(t) for t in x_points_f_simple, f in library_2]

ls_2 = llsq(library_data_2, f_simple_points, bias = false)
f_llsq_2(x) = sum(ls_2[i]*library_2[i](x) for i = 1:length(ls_2))

f_simple_pred = library_data_2 * ls_2
rmse_2 = sqrt(mean(abs2.(f_simple_points - f_simple_pred)))
```

```{julia}
#| echo: false
let
pp_ls = pretty_print(ls_2, library_names_2)

md"""
\begin{subequations} \label{eq:f-2-llsq}
\begin{align} 
  f\_{\text{llsq, 2}}(x) &= $(latexify(pp_ls, env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
    \text{RMSE}[f\_{\text{llsq}}] &= $(latexify(rmse_2, env = :raw, fmt  = FancyNumberFormatter(3)))
\end{align}
\end{subequations}
"""
end
```

This has produced a more parsimonious representation, and Figure \ref{fig:f-2-llsq} shows that the quality of the fit is still excellent. 

```{julia}
#| output: false
#| echo: false
let
x = x_points_f_simple
f1 = [f_llsq_1(x) for x in x_points_f_simple]
f2 = [f_llsq_2(x) for x in x_points_f_simple]

fig = Figure(fontsize = 25)
ax = Axis(fig[1, 1], xlabel = L"x", ylabel = L"f", aspect = AxisAspect(1))
scatter!(ax, x_points_f_simple, f_simple_points, color = :black, markersize = 15, label = L"f(x)")
lines!(ax, x, f1, color = :red,  linewidth = 4, label = L"f_{\text{llsq}}(x)")
lines!(ax, x, f2, color = :blue,  linewidth = 4, label = L"f_{\text{llsq, 2}}(x)")
axislegend(ax, position = :lt)
save(joinpath(@__DIR__, "figures", "f-2-llsq.png"), fig)
end
```

\begin{figure}
    \centering
    \includegraphics[width = 0.5\textwidth]{figures/f-2-llsq.png}
    \caption{The more parsimonious linear least-squares result.}
    \label{fig:f-2-llsq}
\end{figure}

Note that Eqs. \eqref{eq:f-1-llsq} and \eqref{eq:f-2-llsq} show that the RSME has increased by a negligible amount. This calculation contains the essence of the basic sparse representation algorithm: iteratively regress library functions onto the target data and remove small coefficients. 

Before moving on, we will consider one more example which demonstrates the need for a more advanced regression tool than the basic linear least-squares. In this case we will take $g(x) = x + \sin(x) + \cos(x)^2$, see Figure \ref{fig:g-1}. We will therefore augment our library with squared sinusoids.

```{julia}
#| output: false
Random.seed!(1234)
g_simple(x) = x + sin(x) + cos(x)^2
x_points_g = range(-5, 5, length = 20) |> collect
g_points = [g_simple(x) + 0.2*rand() for x in x_points_g]
```


```{julia}
#| output: false
#| echo: false
let
fig = Figure(fontsize = 25)
ax = Axis(fig[1, 1], xlabel = L"x", ylabel = L"g(x) = x + \sin(x) + \cos^2(x)", aspect = AxisAspect(1))
scatter!(ax, x_points_g, g_points, color = :black,  markersize = 15)
save(joinpath(@__DIR__, "figures", "g-1.png"), fig)
end
```

\begin{figure}
    \centering
    \includegraphics[width = 0.5\textwidth]{figures/g-1.png}
    \caption{A simple data set from a scalar function.}
    \label{fig:g-1}
\end{figure}

Provisionally, we might attempt the following solution as per the earlier calculations.

```{julia}
#| output: false
library = [
  x -> 1.0, x -> x, x -> x^2, x -> x^3, 
  x -> sin(x), x -> cos(x), 
  x -> sin(x)^2, x -> cos(x)^2]
library_names_1 = ["1", "x", "x^2", "x^3", "sin(x)", "cos(x)", "sin(x)^2", "cos(x)^2"]
library_data_1 = [f(t) for t in x_points_g, f in library]

# llsq(library_data_1, g_sample_points, bias = false) # will error!
```

The above computation will result in an error on the `llsq` calculation. The culprit is the functions $\sin^2(x)$ and $\cos^2(x)$; they are linearly dependent due to $\sin^2(x) + \cos^2(x) = 1$. The net result is that the library data matrix is ill-conditioned for basic least-squares. Furthermore, in the general case, we do not know which of $\sin^2(x)$ or $\cos^2(x)$ will be the appropriate function and hence would like to include both and have the optimization sort out the details for us. One popular solution to this issue is \emph{ridge regression} \cite{hoerl1970ridge} which introduces an additional parameter $\lambda$ and considers the optimization problem
\begin{equation}
\xi^\star = \underset{\xi}{\text{argmin}}\bigg[ \lVert F - \Theta \xi \rVert_{2} + \lambda \lVert I \xi \rVert_{2} \bigg],
\end{equation}
where $I$ is the identity matrix. We can apply that

```{julia}
#| output: false
rr_g = llsq(library_data_2, f_simple_points, bias = false)
f_llsq_2(x) = sum(rr_g[i]*library_2[i](x) for i = 1:length(rr_g))

f_simple_pred = library_data_2 * rr_g
rmse_2 = sqrt(mean(abs2.(f_simple_points - f_simple_pred)))
```

```{julia}
#| echo: false
let
pp_ls = pretty_print(rr_g, library_names_2)

md"""
\begin{subequations} \label{eq:f-2-llsq}
\begin{align} 
  f\_{\text{llsq, 2}}(x) &= $(latexify(pp_ls, env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
    \text{RMSE}[f\_{\text{llsq}}] &= $(latexify(rmse_2, env = :raw, fmt  = FancyNumberFormatter(3)))
\end{align}
\end{subequations}
"""
end
```



\clearpage

```{julia}
# let
times = x_points_f_simple |> collect
target_data = f_simple_points

library = [x -> 1.0, x -> x, x -> x^2, x -> x^3, x -> sin(x), x -> cos(x)]
library_names = ["1", "x", "x^2", "x^3", "sin(x)", "cos(x)"]
library_data = [f(t) for t in times, f in library]

λ_sparse = 0.5
λ_ridge = 0.1

ls = llsq(library_data, target_data, bias = false)
pp_ls = pretty_print(ls, library_names)

rr = ridge(library_data, target_data, λ_ridge, bias = false)
pp_rr = pretty_print(rr, library_names)

sr = sparse_representation(times, target_data, library_data, λ_sparse = λ_sparse, λ_ridge = λ_ridge)
pp_sr = pretty_print(sr, library_names)

md"""
\begin{subequations} \label{eq:f-simple-1-sparse}
\begin{align} 
  \mathrm{llsq} &= $(latexify(pp_ls, env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
  \mathrm{ridge} &= $(latexify(pp_rr, env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
  \mathrm{sparse} &= $(latexify(pp_sr, env = :raw, fmt  = FancyNumberFormatter(3)))
\end{align}
\end{subequations}
"""
# end
```

As a first example, take $f(x) = x + \sin(x) + \cos(x)^2$. We will add a small noise term to simulate real data.

```{julia}
#| output: false
f_simple(x) = x + sin(x) + cos(x)^2
x_points = range(-5, 5, length = 100)
f_points = [f_simple(x) + 0.2*rand() for x in x_points]
```



- Library functions
- need ridge regression since not positive definite

```{julia}
#| output: false
#| echo: false
let
fig = Figure(fontsize = 25)
ax = Axis(fig[1, 1], xlabel = L"x", ylabel = L"f(x) = x + \sin(x) + \cos(x)^2", aspect = AxisAspect(1))
scatter!(ax, x_points, f_points, color = :black,  markersize = 15)
save(joinpath(@__DIR__, "figures", "test.png"), fig)
end
```

\begin{figure}
    \centering
    \includegraphics[width = 0.5\textwidth]{figures/test.png}
    \caption{test}
    \label{fig:test}
\end{figure}

```{julia}
let
times = x_points |> collect
target_data = f_points

library = [x -> 1.0, x -> x, x -> x^2, x -> x^3, x -> sin(x), x -> cos(x)]
library_names = ["1", "x", "x^2", "x^3", "sin(x)", "cos(x)"]
library_data = [f(t) for t in times, f in library]

λ_sparse = 0.5
λ_ridge = 0.1

rr = ridge(library_data, target_data, λ_ridge, bias = false)
# @info pretty_print(rr, library_names)

sr = sparse_representation(times, target_data, library_data, λ_sparse = λ_sparse, λ_ridge = λ_ridge)
pp = pretty_print(sr, library_names)

md"""
\begin{equation}
$(latexify(pp, env = :raw))
\end{equation}
"""
end
```


## SINDy

## E-SINDy

# Julia Implementation

# Results

## Direct sparse representations

## Fluid

## Slow

## Full

# Conclusions


```{julia}
1 + 2
```

```{julia}
1 + 2
```


<!-- ```{julia}
f0(t) = 1.0
f1(t) = 2*t
f2(t) = sin(t)
f3(t) = f1(t)*f2(t)
f4(t) = cos(t)
f5(t) = f1(t)*f4(t)

times = range(0.0, 10.0, length = 100) |> collect
target_data1 = [f1(t) + 2*f2(t) - 3*f3(t) + 0.1*rand() for t in times]
target_data2 = [4*f1(t) - 2*f4(t) - 5*f3(t) + 0.1*rand() for t in times]
target_data = [target_data1 ;; target_data2]

library = [f0, f1, f2, t -> f1(t)^1.2, f3, t -> f2(t)^2, f4, f5]
library_names = ["", "2t", "sin(t)", "(2t)^1.2", "2t*sin(t)", "sin(t)^2", "cos(t)", "2t*cos(t)"]
library_data = [f(t) for t in times, f in library]

si = sparse_representation(times, target_data, library_data, library_names = library_names, λ_sparse = 0.1, pretty_print = false)
``` -->


<!-- ```{julia}
function lorenz!(du, u, p, t)
    x, y, z = u
    du[1] = 10.0 * (y - x)
    du[2] = x * (28.0 - z) - y
    du[3] = x * y - (8 / 3) * z
end

prob_vec = ODEProblem(lorenz!, [1.0, 1.0, 1.0], (0.0, 10.0))
sol_vec = solve(prob_vec)
``` -->

Blah blah $sdfadf$
\begin{equation}
F = ma
\end{equation}

::: {.callout-tip icon=false}
## Algorithm 1 (stock)

```{julia}
penguins = 1 # <1>
penguins*2 # <2>
```
1. Take `penguins`, and then,
2. add new columns for the bill ratio and bill area...
:::


\begin{algorithm}
\caption{An algorithm with caption}\label{alg:cap}
\begin{algorithmic}
\Require $n \geq 0$
\Ensure $y = x^n$
\State $y \gets 1$
\State $X \gets x$
\State $N \gets n$
\While{$N \neq 0$}
\If{$N$ is even}
    \State $X \gets X \times X$
    \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
\ElsIf{$N$ is odd}
    \State $y \gets y \times X$
    \State $N \gets N - 1$
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}