---
title: "Report on SINDy progress"
author: "Gage Bonner and Michael Castellucci"
latex-tinytex: true
format:
  pdf:
    documentclass: article
    # classoption: [twocolumn]
    keep-tex: true
    toc: true
    number-sections: true
    colorlinks: true
    include-in-header:
      - text: |
          \usepackage{algorithm}
          \usepackage{algpseudocode}
          \usepackage[left=2cm,right=2cm,top = 2cm,bottom = 2cm]{geometry}
          \usepackage[style=alphabetic]{biblatex}
    cite-method: biblatex
# format:
#   html:
#     toc: true
#     html-math-method: mathml
bibliography: refs.bib
engine: julia
julia:
  exeflags: ["--project=/Users/gagebonner/Desktop/Repositories/SINDy-Implementation/"]
highlight-style: arrow
---

***

```{julia}
#| output: false
#| echo: false
include(joinpath(@__DIR__, "..", "sindy2.jl"))
using CairoMakie; set_theme!(theme_latexfonts())
```

# Summary of work

In this project, we investigated the application of *data discovery* algorithms to learn the governing equations for a dynamical system from numerical realizations of their trajectories. We apply the sparse identification of nonlinear dynamics (``SINDy") as originally proposed in \cite{brunton2016discovering} as well as a number of its extensions. The core idea behind these algorithms is that many dynamical systems in science are represented by differential equation systems $\dot{x} = f(x)$ where $f(x)$ often a linear combination of a small number of elementary functions. By contrast, the trajectories $x(t)$ may be extremely complicated and ill-suited to direct fitting. Consider the classic Lorenz system
\begin{subequations} \label{eq:lorenz-def}
\begin{align} 
    \dot{x} &= 10 (y - x), \\
    \dot{y} &= x (28 - z) - y, \\ 
    \dot{z} &= x y - (8 / 3) z .
\end{align}
\end{subequations}
Trajectories $X(t) = (x(t), y(t), z(t))$ of this system are chaotic, making it difficult or impossible to propose a function that fits $X(t)$ directly. On the other hand, $\dot{X}$ is a low order polynomial function of $X(t)$ and hence should be much more tractable in principle. At the highest level, we therefore have the following problem: given numerical trajectories $\{x(t_1), x(t_2), \dots \}$, compute numerically $\{\dot{x}(t_1), \dot{x}(t_2), \dots \}$ and attempt to find a parsimonious (equivalently: \emph{sparse}) combination of simple functions of the $x(t)$ that faithfully represents it. 

This report contains embedded code from the [Julia](https://julialang.org/) programming language.

# Core algorithms

## Sparse representations

Suppose that several values of a function of $f : \mathbb{R} \to \mathbb{R}$ are observed, $\{f(x_1), f(x_2), \dots \}$. A \emph{sparse representation} seeks to represent $f$ as a linear combination of elementary functions of $x$ that contains as few terms as possible while still faithfully representing the behavior of $f$. As a first example, take $f(x) = x + \sin(x)$. We will add a small noise term to simulate real data. This is shown in Figure \ref{fig:f-llsq}.

```{julia}
#| output: false
Random.seed!(1234)
f_simple(x) = x + sin(x)
x_points_f_simple = range(-5, 5, length = 20) |> collect
f_simple_points = [f_simple(x) + 0.2*rand() for x in x_points_f_simple]
```

\begin{figure}
\centering
\begin{subfigure}{0.49\textwidth}
    \includegraphics{figures/f-llsq.png}
    \caption{The function $f(x) = x + \sin(x) + \text{noise}$ and the linear least-squares fit $f_\text{llsq}(x)$ of Eq. \eqref{eq:f-1-llsq}.}
    \label{fig:f-llsq}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \includegraphics{figures/g-ridge.png}
    \caption{The function $g(x) = x + \sin(x) + \cos^2(x) + \text{noise}$ and the ridge regression fit $g_\text{ridge}(x)$ of Eq. \eqref{eq:g-ridge}.}
    \label{fig:g-ridge}
\end{subfigure}
        
\caption{Fitting of two simple functions using linear least-squares and ridge regression.}
\label{fig:fig-sparse-motivation}
\end{figure}

Assuming now that we are given this data with no knowledge of the underlying mechanics, how could this function be represented? Due to the oscillatory nature of the function, we might assume that it may be some linear combination of simple polynomials and sinusoids. Provided this sort of "expert knowledge", we could propose a \emph{library} of functions $L(x)$ that constitute the set of all possible functions that we want to include in our model. In our case, we will take our library to be the vector
\begin{equation}
L(x) = (1, x, x^2, x^3, \sin x, \cos x).
\end{equation}
The problem is therefore to find a vector $\xi \in \mathbb{R}^7$ such that $f(x) \approx \xi \cdot L(x)$. To do this, we will construct an optimization problem whose solution is $\xi$ that takes all of our observational data into account. The \emph{library data} $\Theta$ is given by
\begin{equation}
\Theta(x) 
= 
\begin{pmatrix}
L_1(x_1) & L_2(x_1) & \cdots & L_7(x_1) \\ 
L_1(x_2) & L_2(x_2) & \cdots & L_7(x_2) \\
\vdots   & \vdots   & \ddots & \vdots   \\
L_1(x_N) & L_2(x_N) & \cdots & L_7(x_N) 
\end{pmatrix}
=
\begin{pmatrix}
1 & x_1 & \cdots & \cos(x_1) \\ 
1 & x_2 & \cdots & \cos(x_2) \\
\vdots   & \vdots   & \ddots & \vdots   \\
1 & x_N & \cdots & \cos(x_N) 
\end{pmatrix}.
\end{equation}
Given our \emph{target data} $F = (f(x_1), f(x_2), \dots)$, we naturally have the following optimization problem
\begin{equation}
\xi^\star = \underset{\xi \in \mathbb{R}^7}{\text{argmin}} \lVert F - \Theta \xi \rVert_{2} ,
\end{equation}
where $\Vert\cdot\rVert_2$ indicates the $L_2$ norm and where our final representation is $f(x) \approx L(x) \cdot \xi^\star$. This kind of simple problem is directly amenable to a linear least-squares solution.

```{julia}
#| output: false

library = [x -> 1.0, x -> x, x -> x^2, x -> x^3, x -> sin(x), x -> cos(x)]
library_names = ["1", "x", "x^2", "x^3", "sin(x)", "cos(x)"]
library_data = [f(t) for t in x_points_f_simple, f in library]

ls = llsq(library_data, f_simple_points, bias = false) # from MultivariateStats.jl
f_llsq_1(x) = sum(ls[i]*library[i](x) for i = 1:length(ls))

f_simple_pred = library_data * ls
rmse = sqrt(mean(abs2.(f_simple_points - f_simple_pred)))
```

```{julia}
#| echo: false
let
pp_ls = pretty_print(ls, library_names)

md"""
\begin{subequations} \label{eq:f-1-llsq}
\begin{align} 
  f\_{\text{llsq}}(x) &= $(latexify(pp_ls, env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
  \text{RMSE}[f\_{\text{llsq}}] &= $(latexify(rmse, env = :raw, fmt  = FancyNumberFormatter(3)))
\end{align}
\end{subequations}
"""
end
```

We can see that the fit produced is indeed quite good as in Figure \ref{fig:f-llsq}. However, $f_{\text{llsq}}$ is not sparse, that is, it contains more terms than it necessarily needs to and, in particular, many terms that have small coefficients. Examining Eq. \eqref{eq:f-1-llsq}, we see that the coefficient of $x^3$ is very small. We might therefore try removing it from the library and applying another linear least-squares fit.

```{julia}
#| output: false
library = [x -> 1.0, x -> x, x -> x^2, x -> sin(x), x -> cos(x)]
library_names = ["1", "x", "x^2", "sin(x)", "cos(x)"]
library_data = [f(t) for t in x_points_f_simple, f in library]

ls = llsq(library_data, f_simple_points, bias = false)
f_llsq_2(x) = sum(ls[i]*library[i](x) for i = 1:length(ls))

f_simple_pred = library_data * ls
rmse = sqrt(mean(abs2.(f_simple_points - f_simple_pred)))
```

```{julia}
#| echo: false
let
pp_ls = pretty_print(ls, library_names)

md"""
\begin{subequations} \label{eq:f-2-llsq}
\begin{align} 
  f\_{\text{llsq, 2}}(x) &= $(latexify(pp_ls, env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
    \text{RMSE}[f\_{\text{llsq}}] &= $(latexify(rmse, env = :raw, fmt  = FancyNumberFormatter(3)))
\end{align}
\end{subequations}
"""
end
```

This has produced a more parsimonious representation comparing Eqs. \eqref{eq:f-1-llsq} and \eqref{eq:f-2-llsq} shows that the RSME has increased by a negligible amount. 

```{julia}
#| output: false
#| echo: false
let
x = x_points_f_simple
f1 = [f_llsq_1(x) for x in x_points_f_simple]
# f2 = [f_llsq_2(x) for x in x_points_f_simple]

fig = Figure(fontsize = 25)
ax = Axis(fig[1, 1], xlabel = L"x", aspect = AxisAspect(1))
scatter!(ax, x_points_f_simple, f_simple_points, color = :black, markersize = 15, label = L"f(x)")
lines!(ax, x, f1, color = :red,  linewidth = 3, label = L"f_{\text{llsq}}(x)")
# lines!(ax, x, f2, color = :blue,  linewidth = 4, label = L"f_{\text{llsq, 2}}(x)")
axislegend(ax, position = :lt)
save(joinpath(@__DIR__, "figures", "f-llsq.png"), fig)
end
```

This calculation contains the essence of the basic sparse representation algorithm: iteratively regress library functions onto the target data and remove small coefficients. Before moving on, we will consider one more example which demonstrates the need for a more advanced regression tool than the basic linear least-squares. In this case we will take $g(x) = x + \sin(x) + \cos(x)^2$, see Figure \ref{fig:g-ridge}. 

```{julia}
#| output: false
Random.seed!(1234)
g_simple(x) = x + sin(x) + cos(x)^2
x_points_g = range(-5, 5, length = 20) |> collect
g_points = [g_simple(x) + 0.2*rand() for x in x_points_g]
```

Using expert knowledge of $g(x)$, we will augment our library with squared sinusoids. We might attempt the following solution as per the earlier calculations.

```{julia}
#| output: false
library = [
  x -> 1.0, x -> x, x -> x^2, x -> x^3, 
  x -> sin(x), x -> cos(x), 
  x -> sin(x)^2, x -> cos(x)^2]
library_names = ["1", "x", "x^2", "x^3", "sin(x)", "cos(x)", "sin(x)^2", "cos(x)^2"]
library_data = [f(t) for t in x_points_g, f in library]

# llsq(library_data, g_sample_points, bias = false) # will error!
```

The above computation will result in an error on the `llsq` calculation. The culprits are the functions $\sin^2(x)$ and $\cos^2(x)$; they are linearly dependent due to $\sin^2(x) + \cos^2(x) = 1$. The net result is that the library data matrix is ill-conditioned for basic least-squares. One might wonder why we would include both $\sin^2(x)$ and $cos^2(x)$ in the first place. Here we should recall that the general problem is to find the sparsest representation, i.e. if we can use $\cos^2(x)$ rather than $\sin^2(x) - 1$, this is preferable. One popular solution to the issue of linearly dependent library data is \emph{ridge regression} \cite{hoerl1970ridge} which introduces an additional parameter $\lambda$ and considers the optimization problem
\begin{equation}
\xi^\star = \underset{\xi}{\text{argmin}}\bigg[ \lVert F - \Theta \xi \rVert_{2} + \lambda \lVert I \xi \rVert_{2} \bigg],
\end{equation}
where $I$ is the identity matrix. We can apply this ridge regression with $\lambda = 0.1$

```{julia}
#| output: false
lambda_ridge = 0.1
rr_g = ridge(library_data, g_points, lambda_ridge, bias = false) # from MultivariateStats.jl
g_ridge(x) = sum(rr_g[i]*library[i](x) for i = 1:length(rr_g))

g_pred = library_data * rr_g
rmse = sqrt(mean(abs2.(g_points - g_pred)))
```

```{julia}
#| echo: false
let
pp = pretty_print(rr_g, library_names)

md"""
\begin{subequations} \label{eq:g-ridge}
\scriptsize
\begin{align} 
  g\_{\text{ridge}}(x) &= $(latexify(pp, env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
    \text{RMSE}[g\_{\text{ridge}}] &= $(latexify(rmse, env = :raw, fmt  = FancyNumberFormatter(3)))
\end{align}
\end{subequations}
"""
end
```


```{julia}
#| output: false
#| echo: false
let
x = x_points_g
f1 = [g_ridge(x) for x in x_points_g]

fig = Figure(fontsize = 25)
ax = Axis(fig[1, 1], xlabel = L"x", aspect = AxisAspect(1))
scatter!(ax, x_points_g, g_points, color = :black, markersize = 15, label = L"g(x)")
lines!(ax, x, f1, color = :red,  linewidth = 3, label = L"g_{\text{ridge}}(x)")
axislegend(ax, position = :lt)
save(joinpath(@__DIR__, "figures", "g-ridge.png"), fig)
end
```

We obtain a good fit as shown in Figure \ref{fig:g-ridge}, but it is not sparse. With these motivating examples in hand, we can outline the \emph{sequentially-thresholded ridge regression} algorithm.























\clearpage

```{julia}
# let
times = x_points_f_simple |> collect
target_data = f_simple_points

library = [x -> 1.0, x -> x, x -> x^2, x -> x^3, x -> sin(x), x -> cos(x)]
library_names = ["1", "x", "x^2", "x^3", "sin(x)", "cos(x)"]
library_data = [f(t) for t in times, f in library]

λ_sparse = 0.5
λ_ridge = 0.1

ls = llsq(library_data, target_data, bias = false)
pp_ls = pretty_print(ls, library_names)

rr = ridge(library_data, target_data, λ_ridge, bias = false)
pp_rr = pretty_print(rr, library_names)

sr = sparse_representation(times, target_data, library_data, λ_sparse = λ_sparse, λ_ridge = λ_ridge)
pp_sr = pretty_print(sr, library_names)

md"""
\begin{subequations} \label{eq:f-simple-1-sparse}
\begin{align} 
  \mathrm{llsq} &= $(latexify(pp_ls, env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
  \mathrm{ridge} &= $(latexify(pp_rr, env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
  \mathrm{sparse} &= $(latexify(pp_sr, env = :raw, fmt  = FancyNumberFormatter(3)))
\end{align}
\end{subequations}
"""
# end
```

As a first example, take $f(x) = x + \sin(x) + \cos(x)^2$. We will add a small noise term to simulate real data.

```{julia}
#| output: false
f_simple(x) = x + sin(x) + cos(x)^2
x_points = range(-5, 5, length = 100)
f_points = [f_simple(x) + 0.2*rand() for x in x_points]
```



- Library functions
- need ridge regression since not positive definite

```{julia}
#| output: false
#| echo: false
let
fig = Figure(fontsize = 25)
ax = Axis(fig[1, 1], xlabel = L"x", ylabel = L"f(x) = x + \sin(x) + \cos(x)^2", aspect = AxisAspect(1))
scatter!(ax, x_points, f_points, color = :black,  markersize = 15)
save(joinpath(@__DIR__, "figures", "test.png"), fig)
end
```

\begin{figure}
    \centering
    \includegraphics[width = 0.5\textwidth]{figures/test.png}
    \caption{test}
    \label{fig:test}
\end{figure}

```{julia}
let
times = x_points |> collect
target_data = f_points

library = [x -> 1.0, x -> x, x -> x^2, x -> x^3, x -> sin(x), x -> cos(x)]
library_names = ["1", "x", "x^2", "x^3", "sin(x)", "cos(x)"]
library_data = [f(t) for t in times, f in library]

λ_sparse = 0.5
λ_ridge = 0.1

rr = ridge(library_data, target_data, λ_ridge, bias = false)
# @info pretty_print(rr, library_names)

sr = sparse_representation(times, target_data, library_data, λ_sparse = λ_sparse, λ_ridge = λ_ridge)
pp = pretty_print(sr, library_names)

md"""
\begin{equation}
$(latexify(pp, env = :raw))
\end{equation}
"""
end
```


## SINDy

## E-SINDy

# Julia Implementation

# Results

## Direct sparse representations

## Fluid

## Slow

## Full

# Conclusions


```{julia}
1 + 2
```

```{julia}
1 + 2
```


<!-- ```{julia}
f0(t) = 1.0
f1(t) = 2*t
f2(t) = sin(t)
f3(t) = f1(t)*f2(t)
f4(t) = cos(t)
f5(t) = f1(t)*f4(t)

times = range(0.0, 10.0, length = 100) |> collect
target_data1 = [f1(t) + 2*f2(t) - 3*f3(t) + 0.1*rand() for t in times]
target_data2 = [4*f1(t) - 2*f4(t) - 5*f3(t) + 0.1*rand() for t in times]
target_data = [target_data1 ;; target_data2]

library = [f0, f1, f2, t -> f1(t)^1.2, f3, t -> f2(t)^2, f4, f5]
library_names = ["", "2t", "sin(t)", "(2t)^1.2", "2t*sin(t)", "sin(t)^2", "cos(t)", "2t*cos(t)"]
library_data = [f(t) for t in times, f in library]

si = sparse_representation(times, target_data, library_data, library_names = library_names, λ_sparse = 0.1, pretty_print = false)
``` -->


<!-- ```{julia}
function lorenz!(du, u, p, t)
    x, y, z = u
    du[1] = 10.0 * (y - x)
    du[2] = x * (28.0 - z) - y
    du[3] = x * y - (8 / 3) * z
end

prob_vec = ODEProblem(lorenz!, [1.0, 1.0, 1.0], (0.0, 10.0))
sol_vec = solve(prob_vec)
``` -->

Blah blah $sdfadf$
\begin{equation}
F = ma
\end{equation}

::: {.callout-tip icon=false}
## Algorithm 1 (stock)

```{julia}
penguins = 1 # <1>
penguins*2 # <2>
```
1. Take `penguins`, and then,
2. add new columns for the bill ratio and bill area...
:::


\begin{algorithm}
\caption{An algorithm with caption}\label{alg:cap}
\begin{algorithmic}
\Require $n \geq 0$
\Ensure $y = x^n$
\State $y \gets 1$
\State $X \gets x$
\State $N \gets n$
\While{$N \neq 0$}
\If{$N$ is even}
    \State $X \gets X \times X$
    \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
\ElsIf{$N$ is odd}
    \State $y \gets y \times X$
    \State $N \gets N - 1$
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}