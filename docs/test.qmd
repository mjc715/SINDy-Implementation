---
title: "Report on SINDy progress"
author: "Gage Bonner and Michael Castellucci"
latex-tinytex: true
format:
  pdf:
    documentclass: article
    # classoption: [twocolumn]
    keep-tex: true
    toc: true
    number-sections: true
    colorlinks: true
    include-in-header:
      - text: |
          \usepackage[ruled,vlined]{algorithm2e}
          \usepackage[left=2cm,right=2cm,top = 2cm,bottom = 2cm]{geometry}
          \usepackage[style=alphabetic]{biblatex}
    cite-method: biblatex
bibliography: refs.bib
engine: julia
julia:
  exeflags: ["--project=/Users/gagebonner/Desktop/Repositories/SINDy-Implementation/"]
highlight-style: arrow
---

***

```{julia}
#| output: false
#| echo: false
import Pkg; Pkg.activate(joinpath(@__DIR__, ".."))
using DifferentialEquations
using BSplineKit
using MultivariateStats: llsq, ridge, mean
using Random: seed!
using StatsBase: sample
using Combinatorics: multiexponents
using Markdown
using Latexify
using CairoMakie; set_theme!(theme_latexfonts())
```

```{julia}
#| output: false
#| echo: false
function pretty_print(coeffs::Vector{<:Real}, library_names::Vector{String})
    @assert length(coeffs) == length(library_names)

    nums = string.(round.(coeffs, sigdigits = 3))
    output_string = ""
    for i = 1:length(nums)
        if nums[i] != "0.0"
            output_string *= nums[i] * "*" * library_names[i] * " + "
        end
    end

    output_string = output_string[1:end-3]

    return output_string
end

function pretty_print(coeffs::Vector{<:Vector{<:Real}}, library_names::Vector{String})
    return [pretty_print(coeffs[coeff], library_names) for coeff in coeffs]
end

function pretty_print(coeffs::Matrix{<:Real}, library_names::Vector{String})
    return [pretty_print(coeffs[:,i], library_names) for i = 1:size(coeffs, 2)]
end
```

# Summary of work

In this project, we investigated the application of *data discovery* algorithms to learn the governing equations for a dynamical system from numerical realizations of their trajectories. We apply the sparse identification of nonlinear dynamics (``SINDy") as originally proposed in \cite{brunton2016discovering} as well as a number of its extensions. The core idea behind these algorithms is that many dynamical systems in science are represented by differential equation systems $\dot{x} = f(x)$ where $f(x)$ is  often a linear combination of a small number of elementary functions. By contrast, the trajectories $x(t)$ may be extremely complicated and ill-suited to direct fitting. Consider the classic Lorenz system
\begin{subequations} \label{eq:lorenz-def}
\begin{align} 
    \dot{x} &= 10 (y - x), \\
    \dot{y} &= x (28 - z) - y, \\ 
    \dot{z} &= x y - (8 / 3) z .
\end{align}
\end{subequations}
Trajectories $X(t) = (x(t), y(t), z(t))$ of this system are chaotic, making it difficult to fit $X(t)$ directly. Such a fit can be obtained by spline interpolation, but this generally reveals little about the underlying dynamics. On the other hand, $\dot{X}$ is a low order polynomial function of $X(t)$ and hence should be much more tractable in principle. At the highest level, we therefore have the following problem: given numerical trajectories $\{x(t_1), x(t_2), \dots \}$, compute numerically $\{\dot{x}(t_1), \dot{x}(t_2), \dots \}$ and attempt to find a parsimonious (equivalently: \emph{sparse}) combination of simple functions of the $x(t)$ that faithfully represents it. 

This report contains embedded code from the [Julia](https://julialang.org/) programming language.

# Core algorithms

## Sparse representations

Suppose that several values of a function of $f : \mathbb{R} \to \mathbb{R}$ are observed, $\{f(x_1), f(x_2), \dots \}$. A \emph{sparse representation} seeks to represent $f$ as a linear combination of elementary functions of $x$ that contains as few terms as possible while still faithfully representing the behavior of $f$. As a first example, take $f(x) = x + \sin(x)$. We will add a small noise term to simulate real data. This is shown in Figure \ref{fig:f-llsq}.

```{julia}
#| output: false
seed!(1234)
f_simple(x) = x + sin(x)
x_points_f_simple = range(-5, 5, length = 20) |> collect
f_simple_points = [f_simple(x) + 0.2*rand() for x in x_points_f_simple]
```

\begin{figure}
\centering
\begin{subfigure}{0.49\textwidth}
    \includegraphics{figures/f-llsq.png}
    \caption{The function $f(x) = x + \sin(x) + \text{noise}$ and the linear least-squares fit $f_\text{llsq}(x)$ of Eq. \eqref{eq:f-1-llsq}.}
    \label{fig:f-llsq}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \includegraphics{figures/g-ridge.png}
    \caption{The function $g(x) = x + \sin(x) + \cos^2(x) + \text{noise}$ and the ridge regression fit $g_\text{ridge}(x)$ of Eq. \eqref{eq:g-ridge}.}
    \label{fig:g-ridge}
\end{subfigure}
        
\caption{Fitting of two simple functions using linear least-squares and ridge regression.}
\label{fig:sparse-motivation}
\end{figure}

Assuming now that we are given this data with no knowledge of the underlying mechanics, how could this function be represented? Due to the oscillatory nature of the function, we might assume that it may be some linear combination of simple polynomials and sinusoids. Provided this sort of "expert knowledge", we could propose a \emph{library} of functions $L(x)$ that constitute the set of all possible functions that we want to include in our model. In our case, we will take our library to be the vector
\begin{equation}
L(x) = (1, x, x^2, x^3, \sin x, \cos x).
\end{equation}
The problem is therefore to find a vector $\xi \in \mathbb{R}^7$ such that $f(x) \approx \xi \cdot L(x)$. To do this, we will construct an optimization problem whose solution is $\xi$ that takes all of our observational data into account. The \emph{library data} $\Theta$ is given by
\begin{equation}
\Theta(x) 
= 
\begin{pmatrix}
L_1(x_1) & L_2(x_1) & \cdots & L_7(x_1) \\ 
L_1(x_2) & L_2(x_2) & \cdots & L_7(x_2) \\
\vdots   & \vdots   & \ddots & \vdots   \\
L_1(x_N) & L_2(x_N) & \cdots & L_7(x_N) 
\end{pmatrix}
=
\begin{pmatrix}
1 & x_1 & \cdots & \cos(x_1) \\ 
1 & x_2 & \cdots & \cos(x_2) \\
\vdots   & \vdots   & \ddots & \vdots   \\
1 & x_N & \cdots & \cos(x_N) 
\end{pmatrix}.
\end{equation}
Given our \emph{target data} $F = (f(x_1), f(x_2), \dots)$, we naturally have the following optimization problem
\begin{equation}
\xi^\star = \underset{\xi \in \mathbb{R}^7}{\text{argmin}} \lVert F - \Theta \xi \rVert_{2} ,
\end{equation}
where $\Vert\cdot\rVert_2$ indicates the $L_2$ norm and where our final representation is $f(x) \approx L(x) \cdot \xi^\star$. This kind of simple problem is directly amenable to a linear least-squares solution.

```{julia}
#| output: false

library = [x -> 1.0, x -> x, x -> x^2, x -> x^3, x -> sin(x), x -> cos(x)]
library_names = ["1", "x", "x^2", "x^3", "sin(x)", "cos(x)"]
library_data = [f(t) for t in x_points_f_simple, f in library]

ls = llsq(library_data, f_simple_points, bias = false) # from MultivariateStats.jl
f_llsq_1(x) = sum(ls[i]*library[i](x) for i = 1:length(ls))

f_simple_pred = library_data * ls
rmse = sqrt(mean(abs2.(f_simple_points - f_simple_pred)))
```

```{julia}
#| echo: false
let
pp_ls = pretty_print(ls, library_names)

md"""
\begin{subequations} \label{eq:f-1-llsq}
\begin{align} 
  f\_{\text{llsq}}(x) &= $(latexify(pp_ls, env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
  \text{RMSE}[f\_{\text{llsq}}] &= $(latexify(rmse, env = :raw, fmt  = FancyNumberFormatter(3)))
\end{align}
\end{subequations}
"""
end
```

We can see that the fit produced is indeed quite good as in Figure \ref{fig:f-llsq}. However, $f_{\text{llsq}}$ is not sparse, that is, it contains more terms than it necessarily needs to and, in particular, many terms that have small coefficients. Examining Eq. \eqref{eq:f-1-llsq}, we see that the coefficient of $x^3$ is very small. We might therefore try removing it from the library and applying another linear least-squares fit.

```{julia}
#| output: false
library = [x -> 1.0, x -> x, x -> x^2, x -> sin(x), x -> cos(x)]
library_names = ["1", "x", "x^2", "sin(x)", "cos(x)"]
library_data = [f(t) for t in x_points_f_simple, f in library]

ls = llsq(library_data, f_simple_points, bias = false)
f_llsq_2(x) = sum(ls[i]*library[i](x) for i = 1:length(ls))

f_simple_pred = library_data * ls
rmse = sqrt(mean(abs2.(f_simple_points - f_simple_pred)))
```

```{julia}
#| echo: false
let
pp_ls = pretty_print(ls, library_names)

md"""
\begin{subequations} \label{eq:f-2-llsq}
\begin{align} 
  f\_{\text{llsq, 2}}(x) &= $(latexify(pp_ls, env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
    \text{RMSE}[f\_{\text{llsq}}] &= $(latexify(rmse, env = :raw, fmt  = FancyNumberFormatter(3)))
\end{align}
\end{subequations}
"""
end
```

This has produced a more parsimonious representation and comparing Eqs. \eqref{eq:f-1-llsq} and \eqref{eq:f-2-llsq} shows that the RSME has increased by a negligible amount. 

```{julia}
#| output: false
#| echo: false
let
x = x_points_f_simple
f1 = [f_llsq_1(x) for x in x_points_f_simple]
# f2 = [f_llsq_2(x) for x in x_points_f_simple]

fig = Figure(fontsize = 25)
ax = Axis(fig[1, 1], xlabel = L"x", aspect = AxisAspect(1))
scatter!(ax, x_points_f_simple, f_simple_points, color = :black, markersize = 15, label = L"f(x)")
lines!(ax, x, f1, color = :red,  linewidth = 3, label = L"f_{\text{llsq}}(x)")
# lines!(ax, x, f2, color = :blue,  linewidth = 4, label = L"f_{\text{llsq, 2}}(x)")
axislegend(ax, position = :lt)
save(joinpath(@__DIR__, "figures", "f-llsq.png"), fig)
end
```

This calculation contains the essence of the basic sparse representation algorithm: iteratively regress library functions onto the target data and remove small coefficients. Before moving on, we will consider one more example which demonstrates the need for a more advanced regression tool than the basic linear least-squares. In this case we will take $g(x) = x + \sin(x) + \cos(x)^2$, see Figure \ref{fig:g-ridge}. 

```{julia}
#| output: false
seed!(1234)
g_simple(x) = x + sin(x) + cos(x)^2
x_points_g = range(-5, 5, length = 20) |> collect
g_points = [g_simple(x) + 0.2*rand() for x in x_points_g]
```

Using expert knowledge of $g(x)$, we will augment our library with squared sinusoids. We might attempt the following solution as per the earlier calculations.

```{julia}
#| output: false
library = [
  x -> 1.0, x -> x, x -> x^2, x -> x^3, 
  x -> sin(x), x -> cos(x), 
  x -> sin(x)^2, x -> cos(x)^2]
library_names = ["1", "x", "x^2", "x^3", "sin(x)", "cos(x)", "sin(x)^2", "cos(x)^2"]
library_data = [f(t) for t in x_points_g, f in library]

# llsq(library_data, g_sample_points, bias = false) # will error!
```

The above computation will result in an error on the `llsq` calculation. The culprits are the functions $\sin^2(x)$ and $\cos^2(x)$; they are linearly dependent due to $\sin^2(x) + \cos^2(x) = 1$. The net result is that the library data matrix is ill-conditioned for basic least-squares. One might wonder why we would include both $\sin^2(x)$ and $\cos^2(x)$ in the first place. Here we should recall that the general problem is to find the sparsest representation, i.e. if we can use $\cos^2(x)$ rather than $\sin^2(x) - 1$, this is preferable. One popular solution to the issue of linearly dependent library data is \emph{ridge regression} \cite{hoerl1970ridge} which introduces an additional parameter $\lambda$ and considers the optimization problem
\begin{equation}
\xi^\star = \underset{\xi}{\text{argmin}}\bigg[ \lVert F - \Theta \xi \rVert_{2} + \lambda \lVert I \xi \rVert_{2} \bigg],
\end{equation}
where $I$ is the identity matrix. We can apply this ridge regression with $\lambda = 0.1$

```{julia}
#| output: false
lambda_ridge = 0.1
rr_g = ridge(library_data, g_points, lambda_ridge, bias = false) # from MultivariateStats.jl
g_ridge(x) = sum(rr_g[i]*library[i](x) for i = 1:length(rr_g))

g_pred = library_data * rr_g
rmse = sqrt(mean(abs2.(g_points - g_pred)))
```

```{julia}
#| echo: false
let
pp = pretty_print(rr_g, library_names)

md"""
\begin{subequations} \label{eq:g-ridge}
\scriptsize
\begin{align} 
  g\_{\text{ridge}}(x) &= $(latexify(pp, env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
    \text{RMSE}[g\_{\text{ridge}}] &= $(latexify(rmse, env = :raw, fmt  = FancyNumberFormatter(3)))
\end{align}
\end{subequations}
"""
end
```


```{julia}
#| output: false
#| echo: false
let
x = x_points_g
f1 = [g_ridge(x) for x in x_points_g]

fig = Figure(fontsize = 25)
ax = Axis(fig[1, 1], xlabel = L"x", aspect = AxisAspect(1))
scatter!(ax, x_points_g, g_points, color = :black, markersize = 15, label = L"g(x)")
lines!(ax, x, f1, color = :red,  linewidth = 3, label = L"g_{\text{ridge}}(x)")
axislegend(ax, position = :lt)
save(joinpath(@__DIR__, "figures", "g-ridge.png"), fig)
end
```

We obtain a good fit as shown in Figure \ref{fig:g-ridge}, but it is not sparse. With these motivating examples in hand, we can outline the \emph{sequentially-thresholded ridge regression} (STRidge) algorithm. 


::: {.callout-tip icon=false}
## STRidge algorithm for sparse representation (Julia implementation)

```{julia}
#| output: false
#| label: stridge
function STRidge(
  target_data::Vector{<:Real}, # <1>
  library_data::Matrix{<:Real}; # <2>
  lambda_sparse::Real = 0.1, # <3>
  lambda_ridge::Real = 0.1, # <4>
  max_iters::Integer = 10)

  rr(data) = ridge(data, target_data, lambda_ridge, bias = false) # <5>
  Xi = rr(library_data)

  for _ in 1:max_iters
      smallinds = findall(p -> abs(p) < lambda_sparse, Xi)
      Xi[smallinds] .= 0
      biginds = setdiff(1:length(Xi), smallinds)
      Xi[biginds] = rr(library_data[:, biginds])
  end

  return Xi
end
```
1. `target_data` is a vector of function (observation) values of length $N$.
2. `library_data` is a matrix with a number of columns equal to the number of library functions. Then, each column has $N$ rows, one for the library function evaluated at each observation.
3. `lambda_sparse` is the threshold below which library coefficients are set equal to zero.
4. `lambda_ridge` is the ridge regression regularization parameter.
5. `ridge` is an implementation the ridge regression algorithm from [`MultivariateStats.jl`](https://juliastats.org/MultivariateStats.jl/stable/lreg/#Ridge-Regression).
:::

We see that the core idea behind the `STRidge` function is simply to iteratively threshold small library coefficients and ridge regress the remaining library functions onto the target data. Let us apply this to the $g(x) = x + \sin(x) + \cos^2(x)$ case as above.

```{julia}
#| output: false
g_strr = STRidge(g_points, library_data, lambda_sparse = 0.3, lambda_ridge = 0.1)
```

```{julia}
#| echo: false
let
pp = pretty_print(g_strr, library_names)

md"""
\begin{subequations} \label{eq:g-strr}
\begin{align} 
  g\_{\text{strr}}(x) &= $(latexify(pp, env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
    \text{RMSE}[g\_{\text{strr}}] &= $(latexify(rmse, env = :raw, fmt  = FancyNumberFormatter(3)))
\end{align}
\end{subequations}
"""
end
```

Comparing Eq. \eqref{eq:g-strr} to \eqref{eq:g-ridge} shows that the `STRidge` algorithm has resulted in a significantly more sparse representation without increasing the rmse appreciably. Note that $\lambda_\text{sparse}$ and $\lambda_\text{ridge}$ are not prescribed currently. One has to choose their values judiciously to simultaneously remove small terms and avoid removing large terms.

We will address one final issue before proceding to our discussion of SINDy. What if the target data or observations are multidimensional? As we are restricting our focus to dynamical systems, multidimensional observations require no further calculation as all coordinates will be functions of time. For example, a function such as $f(x, y) = x^2 + y^2$ can be sparely represented by using library functions such as $x(t), x^2(t) \dots, y(t), y^2(t), \dots$ which amounts to increasing the number of columns in $\Theta$. To handle multidimensional observations, we simply apply the `STRidge` algorithm to each dimension of the target data. 

::: {.callout-tip icon=false}
## STRidge algorithm extension to multidimensional observations (Julia implementation)

```{julia}
#| output: false
#| label: stridge-vec
function STRidge(
  target_data::Matrix{<:Real}, # <1>
  library_data::Matrix{<:Real}; 
  lambda_sparse::Real = 0.1,
  lambda_ridge::Real = 0.1,
  max_iters::Integer = 10)

    Xi = zeros(size(library_data, 2), size(target_data, 2))
    for i = 1:size(target_data, 2)
        sr = STRidge(target_data[:, i], library_data, 
            lambda_sparse = lambda_sparse, 
            lambda_ridge = lambda_ridge,
            max_iters = max_iters) # <2>
        Xi[:,i] .= sr
    end

    return Xi
end
```
1. `target_data` is a now a matrix with $N$ rows and a number of columns equal to the dimension of the target.
2. We call the scalar version of `STRidge` on each column of `target_data`.
:::

We will apply this to multidimensional target data using our $f(x)$ and $g(x)$ functions of Figure \ref{fig:sparse-motivation}.

```{julia}
#| output: false
target_data = [f_simple_points ;; g_points]
fg_strr = STRidge(target_data, library_data, lambda_sparse = 0.3, lambda_ridge = 0.1)
```

```{julia}
#| echo: false
let
pp = pretty_print(fg_strr, library_names)

md"""
\begin{subequations} \label{eq:g-strr}
\begin{align} 
  f\_{\text{strr}}(x) &= $(latexify(pp[1], env = :raw, fmt  = FancyNumberFormatter(3))) \\\\
  g\_{\text{strr}}(x) &= $(latexify(pp[2], env = :raw, fmt  = FancyNumberFormatter(3)))
\end{align}
\end{subequations}
"""
end
```

These are the correct sparse representations.

## SINDy

We will now outline the procedure required to apply the sparse representation algorithms discussed previously to dynamical systems. First, we assume that we have systems of the form
\begin{equation} \label{eq:ode-general}
\frac{\text{d}^n X}{\text{d}t^n} = f(t, X,  X', X'', \dots, X^{(n - 1)}),
\end{equation}
where $X(t) = (x(y), y(t), z(t),  \dots))$ is a vector of coordinates and $f$ can be represented by sparse linear combinations of elementary functions. We also assume that the data we have access to are trajectory observations $\{X(t_1), X(t_2), \dots \}$. Eq. \eqref{eq:ode-general} can be readily represented as a system of first order differential equations where all of the nontrivial dynamics are contained in $f$.  If we had access to $\{\dot{X}(t_1), \dot{X}(t_2), \dots \}$ and higher derivatives, then this would immediately result in a sparse representation problem for the function $f$. Hence, we only need one preprocessing function `derivatives` outlined below which will provide the required derivatives via spline interpolation.

::: {.callout-tip icon=false}
## Compute derivatives of a trajectory (Julia implementation)

```{julia}
#| output: false
function derivatives(
  times::Vector{<:Real}, # <1>
  trajectories::Matrix{<:Real}; # <2>
  derivative_order::Integer = 1,
  spline_order::Integer = max(derivative_order + 1, 4)) # <3>

  @assert issorted(times)

  bo = BSplineOrder(spline_order)
  bn = BSplineKit.Natural()

  itps = [spline(interpolate(times, trajectories[:,i], bo, bn))  # <4>
    for i = 1:size(trajectories, 2)]

  data = [[diff(S, Derivative(i))(t) for t in times, S in itps] # <5>
    for i = 0:derivative_order]

  return data
end
```
1. `times` is a vector of (sorted, but possibly non-uniform) times that the trajectory data are given on of length $N$.
2. `trajectories` is a matrix with $N$ rows and a number of columns equal to the number of coordinates.
3. Spline order of 4 corresponds to cubic splines.
4. Spline interpolation using [BSplineKit.jl](https://jipolanco.github.io/BSplineKit.jl/stable/).
5. Construct and return a vector of matrices, where `data[i][:,j]` is the `(i-1)`th derivative of the `j`th coordinate evaluated at `times`. Note that the `0`th derivative is the coordinate itself.
:::

We can therefore treat the data generated by `derivatives` as the `target_data` for our sparse representation algorithm. The final step will be to actually obtain $f$ of Eq. \eqref{eq:ode-general} so that we may integrate it.

Finally we can describe the the standard SINDy algorithm in pseudocode.

::: {.callout-note icon=false}
## SINDy [standard]

1. Let $X$ be an $N \times v$ matrix where the $i^\text{th}$ row is an observation of $v$ variables at time $t_i$. We assume that $X$ satisfies an $n^\text{th}$ order ODE of the form Eq. \eqref{eq:ode-general}.
2. Compute `D = derivatives(t, X, derivative_order = n)`. The last entry of `D` is an $N \times v$ matrix of $\text{d}X^n/\text{d}t^n$ values, which is the `target_data`, i.e. $f$ of Eq. \eqref{eq:ode-general}.
3. Construct `library_data` containing the proposed functions to include in the sparse representation. This is a matrix with $N$ rows and an arbirary number of columns. Each column is in principle any function of $t$ evaluated at each time. Usually the most appropriate functions of $t$ will be functions of the $v$ coordinates. Note that entries of `D` other than the last one are derivatives of order up to $n - 1$ of $X$ and any function of these may also appear in `library_data`.
4. Compute `Xi = STRidge(target_data, library_data; lambda_sparse, lambda_ridge)`, where `lambda_sparse` should be chosen small enough to sparsify the representation but large enough to faithfully represent $X$. 
5. The final sparse representation is given by multiplying `Xi` by the library functions.
:::

Before we test this algorithm, we need some utilities which are not necessarily part of the core SINDy algorithm. To test our sparse representation, we would like to be able to conveniently slot the symbolic terms we obtain into a differential equation solver. We will use [`DifferentialEquations.jl`](https://docs.sciml.ai/DiffEqDocs/latest/) as our main intergrator, which requires that we express our system in first-order form.\footnote{Actually, this is not strictly required, but it is the most general and straightforward way to proceed.} For example, the function we must construct will have the following form for a third-order ODE when there are two variables and we have identified `f1`, `f2`.
```julia
function integrate_me!(du, u, p, t)
  # ode_order = 3, with 2 variables
  # x, y, dx, dy, d2x, d2y, d3x, d3y = u
  du[1:6] = u[7:8]
  du[7] = f1(t, u[1:6])
  du[8] = f2(t, u[1:6])
end
```
Hence, a function like `f2(t, u) = u[1]*u[4]^2` would correspond to a term $y''' = x (y')^2$ in the original ODE. This suggests that the final library functions we create should be of the form `f(t, u)`. Generalizing this pattern, we now define a simple helper function to build this for us, given our library functions and order.

::: {.callout-tip icon=false}
## Generate a function applying library functions to an integrand (Julia implementation)
```{julia}
#| output: false
function library_to_odefun(library::Vector{<:Function}, ode_order::Integer)
  n_vars = length(library)

  if ode_order == 1
    function odefun1!(du, u, p, t)
      for i = 1:n_vars
        du[i] = library[i](t, u)
      end
    end
  else
    n_du = n_vars*ode_order
    idx_v = 1:n_du-n_vars
    idx_f = n_du-n_vars+1:n_du

    function odefunN!(du, u, p, t)
      for i in idx_v
        du[i] = u[i + n_vars]
      end

      for i in idx_f, j = 1:n_vars
        du[i] = library[j](t, u[idx_v])
      end
    end
  end
end
```
:::

The most common library functions are polynomials, but it is cumbersome to create lists of every possible multivariable monomial by hand. We therefore automate this process using the `polynomials` function below. This function will be our primary library creation tool which means that for now we will be restricted to polynomial terms in the coordinates and their derivatives. Arbitrary user-defined functions are still allowed of course, but they must be defined manually. 

::: {.callout-tip icon=false}
## Compute arbitrary polynomial functions of variables and their derivatives (Julia implementation)

```{julia}
#| output: false

function polynomials(
  times::Vector{<:Real},
  trajectories::Matrix{<:Real}, 
  ode_order::Integer, # <1>
  poly_order::Integer; # <2>
  var_names::Union{Vector{<:String}, Nothing} = nothing) # <3>

  data = derivatives(times, trajectories, derivative_order = ode_order - 1)
  data = stack(data, dims = 2) |> 
    x -> reshape(x, size(x, 1), size(x, 2)*size(x, 3))
  
  if var_names === nothing
    var_names = ["x$(i)" for i = 1:size(trajectories, 2)]
  end

  for i = 1:ode_order-1, j = 1:size(trajectories, 2)
    push!(var_names, "d$(i != 1 ? "^$(i)" : "")$(var_names[j])")
  end

  exps = [multiexponents(size(data, 2), i) for i = 0:poly_order] # <4>

  polys = zeros(size(data, 1), sum(length.(exps)))
  funcs = Function[]
  names = fill("", size(polys, 2))

  polys_i = 1
  for order in exps, ex in order
    polys[:,polys_i] .= [data[:,i] .^ ex[i] for i = 1:size(data, 2)] |> 
      x -> prod(stack(x), dims = 2) # <5>

    push!(funcs, (t, v) -> prod(v .^ ex))

    name = "" 
    for i = 1:length(var_names) 
      if ex[i] != 0 
        name *= var_names[i] * (ex[i] != 1 ? "^$(ex[i])" : "") # <6>
      end
    end
    names[polys_i] = name
    polys_i += 1
  end

  names[1] = "1"

  return (polys, names, funcs) # <7>
end
```
1. `ode_order` is equal to $n$ in Eq. \eqref{eq:ode-general}.
2. `poly_order` is an integer equal to the maximum monomial order requested, e.g. if `order == 2` with two variables `x, y`, then `polynomials` will compute `1, x, y, x^2, y^2, xy`.
3. If provided, `var_names` will assign the given names to each variable in its output. Otherwise, defaults `x1, x2, ...` will be used.
4. Multiexponents using [Combinatorics.jl](https://juliamath.github.io/Combinatorics.jl/dev/).
5. Compute each monomial term by raising each column to the appropriate exponent, and then multiply all columns.
6. If an exponent is zero, we don't want its name to show up in that term, and if an exponent is 1, we omit the exponent. Otherwise, the exponent is shown.
7. `polys` is a matrix with $N$ rows, and one column for each monomial evaluated at all times. `names` is a vector of monomial names for pretty printing. `funcs` is a vector of library functions suitable for use in `library_to_odefun`.
:::


```{julia}
ts = rand(5) |> sort
tr = rand(length(ts),  2) # 3 times, 2 variables
ode_order = 2
poly_order = 2 # all monomials up to and including 2

ps = polynomials(ts, tr, ode_order, poly_order, var_names = ["x", "y"])
@info ps[2]
round.(ps[1], sigdigits = 3)
```

\clearpage



```{julia}
function lorenz!(du, u, p, t)
    x, y, z = u
    du[1] = 10.0 * (y - x)
    du[2] = x * (28.0 - z) - y
    du[3] = x * y - (8 / 3) * z
end

sol_lorenz = ODEProblem(lorenz!, [1.0, 1.0, 1.0], (0.0, 100.0)) |> solve

times_lorenz = range(0.0, 10.0, length = 300) |> collect
traj_lorenz = [sol_lorenz(t)[i] for t in times_lorenz, i = 1:3]

ds = derivatives(times_lorenz, traj_lorenz, derivative_order = 1)
x_L, dx_L = ds[1][:,1], ds[2][:,1]
y_L, dy_L = ds[1][:,2], ds[2][:,2]
z_L, dz_L = ds[1][:,3], ds[2][:,3]

target_data = [dx_L ;; dy_L ;; dz_L]
lib_data, lib_names, lib_funcs = polynomials(times_lorenz, traj_lorenz, 1, 5, var_names = ["x", "y", "z"])

rr = STRidge(target_data, lib_data, lambda_sparse = 0.02)
pretty_print(rr, lib_names)
```

```{julia}
f_lib = [(t, u) -> sum([rr[i,j] .* lib_funcs[i](t, u) for i = 1:size(rr, 1)]) for j = 1:size(rr, 2)]
lorenz_sindy = ODEProblem(library_to_odefun(f_lib, 1), [1.0, 1.0, 1.0], (0.0, 100.0)) 
sol_lorenz_sindy = lorenz_sindy |> solve
traj_lorenz_sindy = [sol_lorenz_sindy(t)[i] for t in times_lorenz, i = 1:3]
```

```{julia}
fig = Figure()
ax = Axis(fig[1, 1])
labels = ["x", "y", "z"]
labels_sindy = ["x_SINDy", "y_SINDy", "z_SINDy"]
for i = 1:size(traj_lorenz, 2)
  lines!(ax, times_lorenz, traj_lorenz[:,i], label = labels[i])
end

for i = 1:size(traj_lorenz, 2)
  lines!(ax, times_lorenz, traj_lorenz_sindy[:,i], linestyle = :dash, label = labels_sindy[i])
end

axislegend(ax)

# xlims!(1,20)
fig
```

<!-- SPARSE CUTOFF -->

```{julia}
points = []
rrs = []
n_terms_poss = []
for lambda_sparse in 10 .^ range(-3, 1, length = 1000)
  rr = STRidge(target_data, lib_data, lambda_sparse = lambda_sparse, lambda_ridge = 0.1)
  rmse = sqrt(mean(abs2.(target_data - lib_data * rr)))
  n_terms = count(iszero, rr)

  if !(n_terms in n_terms_poss)
    push!(points, [lambda_sparse, n_terms, rmse])
    push!(rrs, rr)
    push!(n_terms_poss, n_terms)
  else
    points[end] = [lambda_sparse, n_terms, rmse]
    rrs[end] = rr
  end
end
points = stack(points, dims = 1)

fig = Figure()
ax = Axis(fig[1, 1], xlabel = L"\log10(\lambda)", ylabel = "RMSE")
scatter!(ax, log10.(points[:,1]), points[:,3], color = points[:,2])
fig
```

<!-- SAMPLE BAGGING -->

```{julia}
# n_bootstraps = 1000
# lambda_sparse = 0.8
# lambda_ridge = 0.0
# max_iters = 10

# Xi = zeros(size(library_data, 2), size(target_data, 2), n_bootstraps)
# for b = 1:n_bootstraps
#     idx = sample(1:length(times_lorenz), length(times_lorenz), replace = true)
#     Xi_b = STRidge(target_data[idx,:], library_data[idx,:], 
#         lambda_sparse = lambda_sparse,
#         lambda_ridge = lambda_ridge,
#         max_iters = max_iters)

#     Xi[:,:,b] .= Xi_b
# end

# ip = mean(map(x -> x != 0 ? 1.0 : 0.0, Xi), dims = 3)[:,:,1]
```


## E-SINDy

# Julia Implementation

# Results

## Direct sparse representations

## Fluid

## Slow

## Full

# Conclusions
